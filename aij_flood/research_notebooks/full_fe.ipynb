{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1: load water levels and coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# import geopandas as gpd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import features.utils\n",
    "\n",
    "from features.meteo import *\n",
    "\n",
    "from importlib import reload\n",
    "\n",
    "import xarray \n",
    "from siphon.catalog import TDSCatalog\n",
    "\n",
    "from time import time\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = \"../../\"\n",
    "data_dir = os.path.join(root_dir, \"datasets/\")\n",
    "processed_data_dir = os.path.join(data_dir, \"processed_data/\")\n",
    "\n",
    "water_levels_path = os.path.join(data_dir, \"hydro_2018-2020/new_data_all.csv\")\n",
    "posts_path = os.path.join(processed_data_dir, \"asunp.pkl\")\n",
    "\n",
    "meteo_dir = os.path.join(data_dir, \"meteo_new/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/gldsn/.local/share/virtualenvs/aij20-flood-lIcseS5N/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3418, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-5-ad9ba6624e81>\", line 2, in <module>\n",
      "    meteo = meteo_loader.load()\n",
      "  File \"../features/meteo/meteo_loaders.py\", line 18, in load\n",
      "    meteo_data = self._load_concat(filepathes)\n",
      "  File \"../features/meteo/meteo_loaders.py\", line 27, in _load_concat\n",
      "    dfs = load_utils.read_csvs(filepathes)\n",
      "  File \"../features/meteo/load_utils.py\", line 33, in read_csvs\n",
      "    df = reduce_memory_usage(df)\n",
      "  File \"../features/utils.py\", line 49, in reduce_memory_usage\n",
      "    df[col] = df[col].astype(np.float32)\n",
      "  File \"/home/gldsn/.local/share/virtualenvs/aij20-flood-lIcseS5N/lib/python3.8/site-packages/pandas/core/frame.py\", line 3044, in __setitem__\n",
      "    self._set_item(key, value)\n",
      "  File \"/home/gldsn/.local/share/virtualenvs/aij20-flood-lIcseS5N/lib/python3.8/site-packages/pandas/core/frame.py\", line 3120, in _set_item\n",
      "    value = self._sanitize_column(key, value)\n",
      "  File \"/home/gldsn/.local/share/virtualenvs/aij20-flood-lIcseS5N/lib/python3.8/site-packages/pandas/core/frame.py\", line 3745, in _sanitize_column\n",
      "    value = reindexer(value)\n",
      "  File \"/home/gldsn/.local/share/virtualenvs/aij20-flood-lIcseS5N/lib/python3.8/site-packages/pandas/core/frame.py\", line 3726, in reindexer\n",
      "    value = value._values.copy()\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/gldsn/.local/share/virtualenvs/aij20-flood-lIcseS5N/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 2045, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/gldsn/.local/share/virtualenvs/aij20-flood-lIcseS5N/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 1170, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/home/gldsn/.local/share/virtualenvs/aij20-flood-lIcseS5N/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 316, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/gldsn/.local/share/virtualenvs/aij20-flood-lIcseS5N/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 350, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/home/gldsn/.pyenv/versions/3.8.5/lib/python3.8/inspect.py\", line 1503, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"/home/gldsn/.pyenv/versions/3.8.5/lib/python3.8/inspect.py\", line 1461, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"/home/gldsn/.pyenv/versions/3.8.5/lib/python3.8/inspect.py\", line 708, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"/home/gldsn/.pyenv/versions/3.8.5/lib/python3.8/inspect.py\", line 751, in getmodule\n",
      "    f = getabsfile(module)\n",
      "  File \"/home/gldsn/.pyenv/versions/3.8.5/lib/python3.8/inspect.py\", line 721, in getabsfile\n",
      "    return os.path.normcase(os.path.abspath(_filename))\n",
      "  File \"/home/gldsn/.local/share/virtualenvs/aij20-flood-lIcseS5N/lib/python3.8/posixpath.py\", line 381, in abspath\n",
      "    return normpath(path)\n",
      "  File \"/home/gldsn/.local/share/virtualenvs/aij20-flood-lIcseS5N/lib/python3.8/posixpath.py\", line 366, in normpath\n",
      "    path = sep.join(comps)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "object of type 'NoneType' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-ad9ba6624e81>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmeteo_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDirMeteoLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmeteo_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmeteo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmeteo_loader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mmeteo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/aij20-flood/aij_flood/features/meteo/meteo_loaders.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mfilepathes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_find_data_paths\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mmeteo_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_concat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepathes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmeteo_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/aij20-flood/aij_flood/features/meteo/meteo_loaders.py\u001b[0m in \u001b[0;36m_load_concat\u001b[0;34m(self, filepathes)\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_load_concat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilepathes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0mdfs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csvs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepathes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0mconcated_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdfs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/aij20-flood/aij_flood/features/meteo/load_utils.py\u001b[0m in \u001b[0;36mread_csvs\u001b[0;34m(pathes, reduce_mem)\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreduce_mem\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m             \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreduce_memory_usage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/aij20-flood/aij_flood/features/utils.py\u001b[0m in \u001b[0;36mreduce_memory_usage\u001b[0;34m(df, use_float16)\u001b[0m\n\u001b[1;32m     48\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0mc_min\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mc_max\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m                     \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/aij20-flood-lIcseS5N/lib/python3.8/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   3043\u001b[0m             \u001b[0;31m# set column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3044\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_item\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3045\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/aij20-flood-lIcseS5N/lib/python3.8/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_set_item\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   3119\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_valid_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3120\u001b[0;31m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sanitize_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3121\u001b[0m         \u001b[0mNDFrame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_item\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/aij20-flood-lIcseS5N/lib/python3.8/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_sanitize_column\u001b[0;34m(self, key, value, broadcast)\u001b[0m\n\u001b[1;32m   3744\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3745\u001b[0;31m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreindexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3746\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/aij20-flood-lIcseS5N/lib/python3.8/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mreindexer\u001b[0;34m(value)\u001b[0m\n\u001b[1;32m   3725\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mequals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3726\u001b[0;31m                 \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3727\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m~/.local/share/virtualenvs/aij20-flood-lIcseS5N/lib/python3.8/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[1;32m   2044\u001b[0m                         \u001b[0;31m# in the engines. This should return a list of strings.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2045\u001b[0;31m                         \u001b[0mstb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_render_traceback_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2046\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'KeyboardInterrupt' object has no attribute '_render_traceback_'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/aij20-flood-lIcseS5N/lib/python3.8/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[1;32m   2045\u001b[0m                         \u001b[0mstb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_render_traceback_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2046\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2047\u001b[0;31m                         stb = self.InteractiveTB.structured_traceback(etype,\n\u001b[0m\u001b[1;32m   2048\u001b[0m                                             value, tb, tb_offset=tb_offset)\n\u001b[1;32m   2049\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/aij20-flood-lIcseS5N/lib/python3.8/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1434\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1435\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1436\u001b[0;31m         return FormattedTB.structured_traceback(\n\u001b[0m\u001b[1;32m   1437\u001b[0m             self, etype, value, tb, tb_offset, number_of_lines_of_context)\n\u001b[1;32m   1438\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/aij20-flood-lIcseS5N/lib/python3.8/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1334\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose_modes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1335\u001b[0m             \u001b[0;31m# Verbose modes need a full traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1336\u001b[0;31m             return VerboseTB.structured_traceback(\n\u001b[0m\u001b[1;32m   1337\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb_offset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumber_of_lines_of_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1338\u001b[0m             )\n",
      "\u001b[0;32m~/.local/share/virtualenvs/aij20-flood-lIcseS5N/lib/python3.8/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, evalue, etb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1191\u001b[0m         \u001b[0;34m\"\"\"Return a nice text document describing the traceback.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1193\u001b[0;31m         formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n\u001b[0m\u001b[1;32m   1194\u001b[0m                                                                tb_offset)\n\u001b[1;32m   1195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/aij20-flood-lIcseS5N/lib/python3.8/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mformat_exception_as_a_whole\u001b[0;34m(self, etype, evalue, etb, number_of_lines_of_context, tb_offset)\u001b[0m\n\u001b[1;32m   1149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1151\u001b[0;31m         \u001b[0mlast_unique\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecursion_repeat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_recursion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morig_etype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1153\u001b[0m         \u001b[0mframes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat_records\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlast_unique\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecursion_repeat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/aij20-flood-lIcseS5N/lib/python3.8/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mfind_recursion\u001b[0;34m(etype, value, records)\u001b[0m\n\u001b[1;32m    449\u001b[0m     \u001b[0;31m# first frame (from in to out) that looks different.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_recursion_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 451\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m     \u001b[0;31m# Select filename, lineno, func_name to track frames with\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'NoneType' has no len()"
     ]
    }
   ],
   "source": [
    "meteo_loader = DirMeteoLoader(meteo_dir)\n",
    "meteo = meteo_loader.load()\n",
    "meteo.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def object_cols2num(df):\n",
    "    \"\"\"Tries to convert each objct col to num dtype, if error occures for\n",
    "    some col, it's returned not modified\"\"\"\n",
    "    object_cols_mask = (df.dtypes == \"O\").values\n",
    "    object_cols = df.columns[object_cols_mask]\n",
    "    \n",
    "    for col in object_cols:\n",
    "        df[col] = df[col].astype(np.float32, errors=\"ignore\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "water_levels = pd.read_csv(water_levels_path, sep=\";\")\n",
    "posts = features.utils.load_pickle(posts_path)\n",
    "posts = object_cols2num(posts)\n",
    "\n",
    "water_levels = features.utils.reduce_memory_usage(water_levels)\n",
    "\n",
    "water_levels.head(3)\n",
    "posts.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2: form water_levels, manipulating with indexes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.to_datetime(water_levels[\"time\"], yearfirst=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "water_datetime_date = pd.to_datetime(water_levels[\"time\"], yearfirst=True).dt.date\n",
    "water_levels[\"time\"] = water_datetime_date\n",
    "\n",
    "water_levels.rename(columns={\"time\": \"date\", \"identifier\": \"id\"}, inplace=True)\n",
    "\n",
    "water_levels.sort_values(by=[\"id\", \"date\"], inplace=True)\n",
    "water_levels.set_index([\"id\", \"date\"], inplace=True)\n",
    "\n",
    "water_levels.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3: get coords of hydrostations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def coords_by_col_val(post_df, id_colname, needed_values):\n",
    "    id_col_vals = post_df[id_colname]\n",
    "    needed_rows_mask = id_col_vals.isin(needed_values)\n",
    "    needed_rows = post_df[needed_rows_mask.values].reset_index(drop=True)\n",
    "    \n",
    "    coords_with_id_col = needed_rows[[id_colname, \"lon\", \"lat\"]]\n",
    "    return coords_with_id_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "needed_hydro_ids = water_levels.reset_index()[\"id\"].unique()\n",
    "hydro_coords = coords_by_col_val(posts, \"gidro\", needed_hydro_ids)\n",
    "hydro_coords.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hydro_coords.drop_duplicates(subset=\"gidro\", inplace=True)\n",
    "hydro_coords.rename(columns={\"gidro\": \"id\"}, inplace=True)\n",
    "hydro_coords.set_index(\"id\", inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4: meteo processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meteo.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_builder = DtBuilder()\n",
    "meteo_dropper = DirMeteoDropper()\n",
    "diff_cols = [\"cloudCoverTotal\", \"windSpeed\", \"totalAccumulatedPrecipitation\", \"soilTemperature\", \"airTemperature\", \n",
    "                        \"relativeHumidity\", \"pressureReducedToMeanSeaLevel\", \"windAngleX\", \"windAngleY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = DirMeteoPreprocessor(meteo, meteo_dropper, dt_builder, diff_cols)\n",
    "meteo_processed = preprocessor.preprocess()\n",
    "meteo_processed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordinal_extr_stats = [{\"func\": np.nanmean, \"lag\": 1, \"winsize\": 7}, \n",
    "                      {\"func\": np.nanmean, \"lag\": 7, \"winsize\": 30}, \n",
    "                      {\"func\": np.nanstd, \"lag\": 7, \"winsize\": 30}]\n",
    "\n",
    "ordinal_cols = [\"cloudCoverTotal\", \"windSpeed\", \"totalAccumulatedPrecipitation\", \"soilTemperature\",\n",
    "                            \"airTemperature\", \"dewpointTemperature\", \"pressure\", \"pressureReducedToMeanSeaLevel\",\n",
    "                            \"windAngleX\", \"windAngleY\"]\n",
    "\n",
    "min_extr_stats = [{\"func\": np.nanmean, \"lag\": 1, \"winsize\": 7}]\n",
    "min_cols = [\"minimumTemperatureAtHeightAndOverPeriodSpecified\", \"maximumTemperatureOverPeriodSpecified\"]\n",
    "\n",
    "#!!!!!!!!!!!!! add diff cols in min cols\n",
    "\n",
    "config = ExtrConfig(meteo_processed.columns)\n",
    "config.update_with_config_and_cols(ordinal_extr_stats, ordinal_cols)\n",
    "config.update_with_config_and_cols(min_extr_stats, min_cols)\n",
    "extr_config = config.get_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(pd.DataFrame(np.arange(20).reshape(4, 5))[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meteo_extract = MeteoExtractManager(meteo_processed, extr_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meteo_extract.agg_daily()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meteo_extract.stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meteo_extract.get_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# get meteo forecast data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weather_forecast(points: list, required_data):\n",
    "    url = \"http://thredds.ucar.edu/thredds/catalog/grib/NCEP/GFS/Global_0p5deg/catalog.xml\"\n",
    "    point_outputs = []\n",
    "    \n",
    "    client = get_latest_dataset_client(url)\n",
    "    query = build_base_query(client, required_data)\n",
    "    \n",
    "    for _, point in points.iterrows():\n",
    "        query = update_query_point(query, point)\n",
    "        print(\"get data, point: \", point)#, query)\n",
    "        t1 = time()\n",
    "        out = client.get_data(query)\n",
    "        print(\"Got data, time:\", time() - t1)\n",
    "        point_outputs.append(out)\n",
    "    \n",
    "    return point_outputs\n",
    "\n",
    "def get_latest_dataset_client(catalog_url):\n",
    "    gfs_cat = TDSCatalog(catalog_url)\n",
    "    ncss_client = gfs_cat.latest.subset()\n",
    "    return ncss_client\n",
    "\n",
    "\n",
    "def build_base_query(client, required_data):\n",
    "    query = client.query()\n",
    "    \n",
    "    query = query.variables(*required_data)\n",
    "    query = query.all_times()\n",
    "    query = query.accept(\"netCDF4\")\n",
    "\n",
    "    return query\n",
    "\n",
    "\n",
    "def update_query_point(query, point): #also deletes old point\n",
    "    \n",
    "    return query\n",
    "\n",
    "# forecast = get_weather_forecast\n",
    "forecast_url = \"http://thredds.ucar.edu/thredds/catalog/grib/NCEP/GFS/Global_0p5deg/catalog.xml\"\n",
    "catalog = TDSCatalog(forecast_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ForecastMeteoLoader:\n",
    "    def __init__(self, start_date, end_date, meteo_coords, retrieved_vars, varnames_table: pd.DataFrame, parser):\n",
    "        \"\"\"\n",
    "        :param varnames_table: df with 1 col containing source name and 2 col with forecast names\n",
    "        :param meteo_coords: contains id, lon and lat cols\n",
    "        \"\"\"\n",
    "        \n",
    "        self.start_date = start_date\n",
    "        self.end_date = end_date\n",
    "        self.meteo_coords = meteo_coords\n",
    "        self.parser = parser\n",
    "        \n",
    "        self.varnames_table = varnames_table\n",
    "        \n",
    "        self.forecast_url = \"http://thredds.ucar.edu/thredds/catalog/grib/NCEP/GFS/Global_0p5deg/catalog.xml\"\n",
    "        self.catalog = TDSCatalog(self.forecast_url)\n",
    "        \n",
    "        self.ncss_client = self._get_remote_client()\n",
    "        self.retrieved_vars = self._convert_var_names(retrieved_vars)\n",
    "\n",
    "    def load(self):\n",
    "        raw_forecasts = self._download_forecast()\n",
    "        \n",
    "        points_df = []\n",
    "        for point_forecast in raw_forecasts:\n",
    "            df_point = self.parser.parse(point_forecast, self.retrieved_vars)\n",
    "            points_df.append(df_point)\n",
    "            \n",
    "        \n",
    "        df = self._build_general_df(points_df)\n",
    "        df = self._filter_dates(df)\n",
    "        print(\"set nortmal colnames\")\n",
    "        df = self._set_normal_colnames(df)\n",
    "        return df\n",
    "        \n",
    "    def _get_remote_client(self):\n",
    "        datasets = self.catalog.catalog_refs\n",
    "        data_dates = self._extract_datasets_dates(datasets)\n",
    "        idx_nearest_ds = self._nearest_date_idx(data_dates, start_date)\n",
    "        nearest_dataset = datasets[idx_nearest_ds]\n",
    "        \n",
    "        #print(nearest_dataset, type(nearest_dataset))\n",
    "        ncss_client = nearest_dataset.follow().datasets[0].subset() # clean later\n",
    "        return ncss_client\n",
    "    \n",
    "    def _extract_datasets_dates(self, datasets):\n",
    "        dates = []\n",
    "    \n",
    "        for ds_name in datasets.keys():\n",
    "            str_date = ds_name.split(\"_\")[3]\n",
    "            ds_date = datetime.strptime(str_date, \"%Y%m%d\")\n",
    "\n",
    "            dates.append(ds_date)\n",
    "\n",
    "        return dates\n",
    "    \n",
    "    def _nearest_date_idx(self, dates, compared_date):\n",
    "        time_diffs = []\n",
    "    \n",
    "        for data_date in dates:\n",
    "            diff = abs(data_date - compared_date)\n",
    "            time_diffs.append(diff)\n",
    "\n",
    "        return np.argmin(time_diffs)\n",
    "    \n",
    "    def _download_forecast(self):\n",
    "        point_outputs = []\n",
    "        \n",
    "        for _, point in meteo_coords.iterrows():\n",
    "            query = self._build_base_query(point)\n",
    "            print(\"get data\", point)\n",
    "            t1 = time()\n",
    "            out = self.ncss_client.get_data(query)\n",
    "            print(\"got data\", time() - t1)\n",
    "            point_outputs.append(out)\n",
    "            \n",
    "        return point_outputs\n",
    "    \n",
    "    def _build_base_query(self, point):\n",
    "        query = self.ncss_client.query()\n",
    "        #print(*self.retrieved_vars)\n",
    "        #print(point[\"lon\"], point[\"lat\"])\n",
    "        query.variables(*self.retrieved_vars)\n",
    "#         query = query.all_times()\n",
    "        query.time_range(self.start_date, self.end_date)\n",
    "        query.lonlat_point(point[\"lon\"], point[\"lat\"])\n",
    "        \n",
    "        query.accept(\"netCDF4\")\n",
    "\n",
    "        return query\n",
    "    \n",
    "    def _convert_var_names(self, src_names):\n",
    "        src2forecast_names = self.varnames_table.reset_index().set_index(\"src\")\n",
    "        new_names = [src2forecast_names.loc[name, \"forecast\"] for name in src_names]\n",
    "        return new_names\n",
    "    \n",
    "    def _set_normal_colnames(self, df):\n",
    "        forecast2src_names = self.varnames_table.reset_index().set_index(\"forecast\")\n",
    "        forecast_vars = forecast2src_names.index\n",
    "        print('eeee')\n",
    "        rename_dict = {forecast_name: forecast2src_names.loc[forecast_name, \"src\"] for forecast_name in forecast_vars}\n",
    "        print(\"EEEEE\", rename_dict)\n",
    "        df.rename(columns=rename_dict, inplace=True)\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def _filter_dates(self, df):\n",
    "        print(\"filter dates enter\")\n",
    "        datetime = pd.Index(df.reset_index()[\"datetime\"]).tz_localize(None)\n",
    "        print(\"mask created\")\n",
    "        in_needed_range_mask = (datetime >= self.start_date) & (datetime <= self.end_date)\n",
    "        \n",
    "        return df[in_needed_range_mask]\n",
    "    \n",
    "    def _build_general_df(self, dfs):\n",
    "        print(\"build general\")\n",
    "        ids = list(self.meteo_coords[\"id\"])\n",
    "        print(\"aa\")\n",
    "        for station_id, df in zip(ids, dfs):\n",
    "            df[\"id\"] = station_id\n",
    "        print(\"cc\")\n",
    "        general_df = pd.concat(dfs, axis=0)\n",
    "        return general_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: check fill_forecast_nan() from meteo_extraction.ipynb\n",
    "class ForecastParser:\n",
    "    def parse(self, forecast, var_names):\n",
    "        print(\"parsing\")\n",
    "        self.data = xarray.backends.NetCDF4DataStore(forecast)\n",
    "    \n",
    "        values = {}\n",
    "        for name in var_names:\n",
    "            var_value = self._var_val(name)\n",
    "            values[name] = var_value\n",
    "            \n",
    "        index = self._get_timestamps()\n",
    "        df = pd.DataFrame.from_dict(values, orient=\"columns\")\n",
    "        df.index = index\n",
    "        \n",
    "        return df\n",
    "            \n",
    "    def _var_val(self, name):\n",
    "        values = self.data.get_variables()[name].values\n",
    "        return self._reshape_var_val(values)\n",
    "    \n",
    "    def _reshape_var_val(self, vals):\n",
    "        shape = vals.shape\n",
    "        \n",
    "        if len(shape) == 3:\n",
    "            vals = vals[:, :, 0]\n",
    "        elif len(shape) > 3:\n",
    "            raise ValueError(\"В Forecast размерность values > 3\")\n",
    "\n",
    "        return vals.flatten()[:-1] # OMG WTF DOES THIS SLICE MEAN!? check later and clean code\n",
    "    \n",
    "    def _get_timestamps(self):\n",
    "        attrs = self.data.get_attrs()\n",
    "        start_time = pd.to_datetime(attrs[\"time_coverage_start\"])\n",
    "\n",
    "        hours_from_start = self.data.get_variables()[\"time\"]\n",
    "        time_deltas = self._timedelta_from_hours(hours_from_start)[:-1] #same slice \n",
    "\n",
    "        timestamps = start_time + time_deltas\n",
    "        timestamps.name = \"datetime\"\n",
    "        \n",
    "        return timestamps\n",
    "    \n",
    "    def _timedelta_from_hours(self, hours):\n",
    "        return pd.to_timedelta(hours.values.flatten(), \"h\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ForecastMeteoUnifier:\n",
    "    def __init__(self):\n",
    "        self.cloud_cover_col = \"cloudCoverTotal\"\n",
    "        self.x_wind_col = 'windAngleX'\n",
    "        self.y_wind_col = 'windAngleY'\n",
    "        \n",
    "        self.temperature_cols = [\"airTemperature\", \"soilTemperature\",\n",
    "                                 \"maximumTemperatureOverPeriodSpecified\",\n",
    "                                 \"minimumTemperatureAtHeightAndOverPeriodSpecified\",\n",
    "                                 \"dewpointTemperature\"\n",
    "                                ]\n",
    "        \n",
    "        self.pressure_cols = [\"pressure\", \"pressureReducedToMeanSeaLevel\"] \n",
    "    \n",
    "    def unify(self, forecast_df):\n",
    "        rescaled_df = self.rescale(forecast_df)\n",
    "        return rescaled_df\n",
    "        \n",
    "    def rescale(self, df):\n",
    "        df = self._rescale_cloud_cover(df)\n",
    "        df = self._rescale_wind_vector(df)\n",
    "        df = self._rescale_temperature(df)\n",
    "        df = self._rescale_pressure(df)\n",
    "        return df\n",
    "        \n",
    "    def _rescale_cloud_cover(self, df):\n",
    "        df[self.cloud_cover_col] /= 10 # rescaling from 0-100 to 0-10\n",
    "        return df\n",
    "\n",
    "    def _rescale_wind_vector(self, df):\n",
    "        wind_x = df[self.x_wind_col]\n",
    "        wind_y = df[self.y_wind_col]\n",
    "        vector_module = (wind_x ** 2 + wind_y ** 2)**0.5\n",
    "\n",
    "        wind_x = np.sign(wind_x) * wind_x / vector_module\n",
    "        wind_y = np.sign(wind_y) * wind_y / vector_module\n",
    "\n",
    "        # if vector_module == 0, wind_x and wind_y are np.nan\n",
    "        wind_x[vector_module == 0] = 0\n",
    "        wind_y[vector_module == 0] = 0\n",
    "\n",
    "\n",
    "        df[self.x_wind_col] = wind_x\n",
    "        df[self.y_wind_col] = wind_y\n",
    "\n",
    "        return df\n",
    "\n",
    "    def _rescale_temperature(self, df):\n",
    "        df[self.temperature_cols] -= 273 # temperature there is in absolute form\n",
    "        return df\n",
    "\n",
    "    def _rescale_pressure(self, df):\n",
    "        df[self.pressure_cols] /= 100\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = datetime.strptime(\"13-11-2020\", \"%d-%m-%Y\")\n",
    "end_date = start_date + timedelta(days=10)\n",
    "\n",
    "parser = ForecastParser()\n",
    "\n",
    "name_forecast_src = np.array([(\"Total_cloud_cover_entire_atmosphere_Mixed_intervals_Average\", \"cloudCoverTotal\"),\n",
    "                        ('u-component_of_wind_height_above_ground', \"windAngleX\"), \n",
    "                        ('v-component_of_wind_height_above_ground', \"windAngleY\"),\n",
    "                        ('Wind_speed_gust_surface', \"windSpeed\"), \n",
    "                        ('Total_precipitation_surface_Mixed_intervals_Accumulation', \"totalAccumulatedPrecipitation\"), \n",
    "                        (\"Temperature_height_above_ground\", 'airTemperature'), \n",
    "                        ('Maximum_temperature_height_above_ground_Mixed_intervals_Maximum', 'maximumTemperatureOverPeriodSpecified'), \n",
    "                        ('Minimum_temperature_height_above_ground_Mixed_intervals_Minimum', 'minimumTemperatureAtHeightAndOverPeriodSpecified'),\n",
    "                        ('Temperature_surface', 'soilTemperature'), \n",
    "                        ('Relative_humidity_height_above_ground', 'relativeHumidity'), \n",
    "                        ('Pressure_height_above_ground', 'pressure'), \n",
    "                        ('Pressure_reduced_to_MSL_msl', 'pressureReducedToMeanSeaLevel'),\n",
    "                        (\"Dewpoint_temperature_height_above_ground\", \"dewpointTemperature\")\n",
    "                       ])\n",
    "\n",
    "retrieved_vars = list(name_forecast_src[:, 1])\n",
    "varnames_table = pd.DataFrame(name_forecast_src, columns=[\"forecast\", \"src\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: сделать meteo_coords\n",
    "\n",
    "meteo_coords = pd.DataFrame({\"id\": [30001, 30002], \"lon\": [123, 127], \"lat\": [53, 50]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_loader = ForecastMeteoLoader(start_date, end_date, meteo_coords, retrieved_vars, varnames_table, parser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast = forecast_loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_unifier = ForecastMeteoUnifier()\n",
    "forecast_unified = forecast_unifier.unify(forecast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_unified.rename(columns={\"id\": \"stationNumber\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_extractor = MeteoExtractManager(forecast_unified, extr_config)\n",
    "\n",
    "forecast_extractor.agg_daily()\n",
    "forecast_extractor.stats()\n",
    "forecast_features = forecast_extractor.get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Features:\n",
    "    def __init__(self, meteo_loader, hydro_path):\n",
    "        self.meteo_loader = meteo_loader\n",
    "        self.hydro_path = hydro_path\n",
    "    \n",
    "    def load_from_file(self):\n",
    "        self.meteo = self.meteo_loader.load()\n",
    "        self.hydro = self._simple_load_hydro()\n",
    "        \n",
    "    def _simple_load_hydro(self):\n",
    "        hydro = pd.read_csv(self.hydro_path, sep=\";\")\n",
    "        hydro = features.reduce_memory_usage(hydro)\n",
    "        \n",
    "        hydro_date = pd.to_datetime(hydro[\"time\"], yearfirst=True).dt.date\n",
    "        hydro[\"time\"] = hydro_date\n",
    "\n",
    "        hydro.rename(columns={\"time\": \"date\", \"identifier\": \"id\"}, inplace=True)\n",
    "\n",
    "        hydro.sort_values(by=[\"id\", \"date\"], inplace=True)\n",
    "        hydro.set_index([\"id\", \"date\"], inplace=True)\n",
    "        \n",
    "        return hydro\n",
    "        \n",
    "        \n",
    "    # TODO: загрузка данных из файлов\n",
    "    # TODO: предобработка и получение фич meteo и hydro \n",
    "    # TODO: мердж meteo и hydro\n",
    "    # TODO: merge по коррелирующим и ближайшим\n",
    "    # TODO: нахождение кластеров, аггрегация среднего global и по кластеру\n",
    "    # TODO: создание фич для test в step predict\n",
    "    # + прикрутить уже готовый data loader/ data retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def E(a, b):\n",
    "    for i in range(5):\n",
    "        yield a+i, b+i*2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "E(1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
